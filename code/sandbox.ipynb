{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperProcessor\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from config import WhiSBERTConfig, CACHE_DIR, CHECKPOINT_DIR\n",
    "from model import WhiSBERTModel\n",
    "from data import AudioDataset, collate_train\n",
    "from train import load_models\n",
    "from utils import (\n",
    "    mean_pooling,\n",
    "    cos_sim_loss,\n",
    "    sim_clr_loss,\n",
    "    norm_temp_ce_loss\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperDecoder(\n",
       "  (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "  (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x WhisperDecoderLayer(\n",
       "      (self_attn): WhisperSdpaAttention(\n",
       "        (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): WhisperSdpaAttention(\n",
       "        (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "        (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "      (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "      (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from config import CACHE_DIR\n",
    "from model import (\n",
    "    expand_conv1d_layer,\n",
    "    expand_embedding_layer,\n",
    "    expand_linear_layer,\n",
    "    expand_layer_norm,\n",
    "    expand_positional_embedding\n",
    ")\n",
    "from transformers import WhisperModel\n",
    "\n",
    "whisper_model = WhisperModel.from_pretrained(\n",
    "    'openai/whisper-tiny',\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "n_new_dims = 7\n",
    "whisper_model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperPositionalEmbedding(448, 384)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0041, -0.0136,  0.0009,  ..., -0.0028, -0.0451,  0.0170],\n",
      "        [ 0.0212, -0.0119,  0.0017,  ...,  0.0055, -0.0307, -0.0371],\n",
      "        [ 0.0233, -0.0196,  0.0041,  ...,  0.0133, -0.0091, -0.0507],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0008,  0.0019,  ..., -0.0101,  0.0593, -0.0149],\n",
      "        [-0.0064, -0.0149,  0.0009,  ..., -0.0015,  0.0635, -0.0135],\n",
      "        [-0.0016, -0.0101,  0.0026,  ..., -0.0035,  0.0626, -0.0055]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(whisper_model.decoder.embed_positions)\n",
    "print(whisper_model.decoder.embed_positions.weight)\n",
    "# print(whisper_model.encoder.embed_positions.weight.shape)\n",
    "# print(whisper_model.encoder.embed_positions.weight.dtype)\n",
    "# print(whisper_model.encoder.embed_positions.weight[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.decoder.embed_tokens = expand_embedding_layer(whisper_model.decoder.embed_tokens, n_new_dims, distribution='normal')\n",
    "whisper_model.decoder.embed_positions = expand_positional_embedding(whisper_model.decoder.embed_positions, n_new_dims)\n",
    "\n",
    "for layer in whisper_model.decoder.layers:\n",
    "    layer.self_attn.k_proj = expand_linear_layer(layer.self_attn.k_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn.v_proj = expand_linear_layer(layer.self_attn.v_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn.q_proj = expand_linear_layer(layer.self_attn.q_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn.out_proj = expand_linear_layer(layer.self_attn.out_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn_layer_norm = expand_layer_norm(layer.self_attn_layer_norm, n_new_dims)\n",
    "    layer.encoder_attn.k_proj = expand_linear_layer(layer.encoder_attn.k_proj, n_new_dims, n_new_dims)\n",
    "    layer.encoder_attn.v_proj = expand_linear_layer(layer.encoder_attn.v_proj, n_new_dims, n_new_dims)\n",
    "    layer.encoder_attn.q_proj = expand_linear_layer(layer.encoder_attn.q_proj, n_new_dims, n_new_dims)\n",
    "    layer.encoder_attn.out_proj = expand_linear_layer(layer.encoder_attn.out_proj, n_new_dims, n_new_dims)\n",
    "    layer.encoder_attn_layer_norm = expand_layer_norm(layer.encoder_attn_layer_norm, n_new_dims)\n",
    "    layer.fc1 = expand_linear_layer(layer.fc1, added_in_features=n_new_dims)\n",
    "    layer.fc2 = expand_linear_layer(layer.fc2, added_out_features=n_new_dims)\n",
    "    layer.final_layer_norm = expand_layer_norm(layer.final_layer_norm, n_new_dims)\n",
    "\n",
    "whisper_model.decoder.layer_norm = expand_layer_norm(whisper_model.decoder.layer_norm, n_new_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhisperPositionalEmbedding(448, 391)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0041, -0.0136,  0.0009,  ..., -0.0155,  0.0111,  0.0084],\n",
      "        [ 0.0212, -0.0119,  0.0017,  ...,  0.0139,  0.0174,  0.0117],\n",
      "        [ 0.0233, -0.0196,  0.0041,  ..., -0.0025, -0.0043,  0.0102],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0008,  0.0019,  ..., -0.0011, -0.0084,  0.0098],\n",
      "        [-0.0064, -0.0149,  0.0009,  ..., -0.0041, -0.0003, -0.0071],\n",
      "        [-0.0016, -0.0101,  0.0026,  ...,  0.0124, -0.0141,  0.0017]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(whisper_model.decoder.embed_positions)\n",
    "print(whisper_model.decoder.embed_positions.weight)\n",
    "# print(whisper_model.encoder.embed_positions.weight.shape)\n",
    "# print(whisper_model.encoder.embed_positions.weight.dtype)\n",
    "# print(whisper_model.encoder.embed_positions.weight[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperDecoder(\n",
       "  (embed_tokens): Embedding(51865, 391, padding_idx=50257)\n",
       "  (embed_positions): WhisperPositionalEmbedding(448, 391)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x WhisperDecoderLayer(\n",
       "      (self_attn): WhisperSdpaAttention(\n",
       "        (k_proj): Linear(in_features=391, out_features=391, bias=False)\n",
       "        (v_proj): Linear(in_features=391, out_features=391, bias=True)\n",
       "        (q_proj): Linear(in_features=391, out_features=391, bias=True)\n",
       "        (out_proj): Linear(in_features=391, out_features=391, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((391,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder_attn): WhisperSdpaAttention(\n",
       "        (k_proj): Linear(in_features=391, out_features=391, bias=False)\n",
       "        (v_proj): Linear(in_features=391, out_features=391, bias=True)\n",
       "        (q_proj): Linear(in_features=391, out_features=391, bias=True)\n",
       "        (out_proj): Linear(in_features=391, out_features=391, bias=True)\n",
       "      )\n",
       "      (encoder_attn_layer_norm): LayerNorm((391,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=391, out_features=1536, bias=True)\n",
       "      (fc2): Linear(in_features=1536, out_features=391, bias=True)\n",
       "      (final_layer_norm): LayerNorm((391,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((391,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model.encoder.conv1 = expand_conv1d_layer(whisper_model.encoder.conv1, added_out_channels=n_new_dims)\n",
    "whisper_model.encoder.conv2 = expand_conv1d_layer(whisper_model.encoder.conv2, added_in_channels=n_new_dims, added_out_channels=n_new_dims)\n",
    "\n",
    "whisper_model.encoder.embed_positions = expand_embedding_layer(whisper_model.encoder.embed_positions, n_new_dims, distribution='zeros')\n",
    "whisper_model.encoder.embed_positions.weight.requires_grad = False\n",
    "\n",
    "for layer in whisper_model.encoder.layers:\n",
    "    layer.self_attn.k_proj = expand_linear_layer(layer.self_attn.k_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn.v_proj = expand_linear_layer(layer.self_attn.v_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn.q_proj = expand_linear_layer(layer.self_attn.q_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn.out_proj = expand_linear_layer(layer.self_attn.out_proj, n_new_dims, n_new_dims)\n",
    "    layer.self_attn_layer_norm = expand_layer_norm(layer.self_attn_layer_norm, n_new_dims)\n",
    "    layer.fc1 = expand_linear_layer(layer.fc1, added_in_features=n_new_dims)\n",
    "    layer.fc2 = expand_linear_layer(layer.fc2, added_out_features=n_new_dims)\n",
    "    layer.final_layer_norm = expand_layer_norm(layer.final_layer_norm, n_new_dims)\n",
    "\n",
    "whisper_model.encoder.layer_norm = expand_layer_norm(whisper_model.encoder.layer_norm, n_new_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = WhiSBERTConfig(\n",
    "    whisper_model_id='openai/whisper-base',\n",
    "    pooling_mode='mean',\n",
    "    use_sbert_encoder=True,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    device='cuda'\n",
    ")\n",
    "processor, _, _, _ = load_models(config, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing AudioDataset...\n",
      "\tTotal dataset size (N): 50369\n",
      "\tTraining dataset size (N): 40295\n",
      "\tValidation dataset size (N): 10074\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing AudioDataset...')\n",
    "dataset = AudioDataset(processor)\n",
    "mini_size = int(0.1 * len(dataset))\n",
    "drop_size = len(dataset) - mini_size\n",
    "mini_dataset, _ = torch.utils.data.random_split(dataset, [mini_size, drop_size])\n",
    "\n",
    "# Calculate lengths for the train/val split (80:20)\n",
    "total_size = len(mini_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "# Perform the split\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mini_dataset, [train_size, val_size])\n",
    "print(f'\\tTotal dataset size (N): {total_size}')\n",
    "print(f'\\tTraining dataset size (N): {train_size}')\n",
    "print(f'\\tValidation dataset size (N): {val_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=config.shuffle,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 80, 3000])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print(batch['audio_inputs'].shape)\n",
    "print(len(batch['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt').to(config.device)\n",
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 28, 768])\n",
      "torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sbert_output = sbert(**encoded_input)\n",
    "print(sbert_output.last_hidden_state.shape)\n",
    "sentence_embeddings = mean_pooling(sbert_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 22, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = sbert.embeddings(input_ids=encoded_input['input_ids'])#, attn_mask=encoded_input['attention_mask'])\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 22, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask = [None] * sbert.config.num_hidden_layers\n",
    "encoder_output = sbert.transformer(\n",
    "    embedding_output,\n",
    "    attn_mask=torch.ones(encoded_input['input_ids'].size(), device=config.device),\n",
    "    head_mask=head_mask\n",
    ")[0]\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel.from_pretrained(\n",
    "    config.whisper_model_id,\n",
    "    cache_dir=CACHE_DIR\n",
    ").to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 27])\n",
      "torch.Size([8, 27])\n"
     ]
    }
   ],
   "source": [
    "# Whisper-based tokenization\n",
    "with torch.no_grad():\n",
    "    outputs = processor.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    ).to(config.device)\n",
    "print(outputs['input_ids'].shape)\n",
    "print(outputs['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = whisper_model(\n",
    "    batch['audio_inputs'].to(config.device),\n",
    "    decoder_input_ids=outputs['input_ids'],\n",
    "    decoder_attention_mask=outputs['attention_mask']\n",
    ").last_hidden_state\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], whisper_embs.size()[:-1])\n",
    "# extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], outputs['attention_mask'].size())\n",
    "encoder_output = sbert_model.transformer(embedding_output, attn_mask=torch.ones(encoded_input['input_ids'].size(), device=config.device), head_mask=head_mask)[0]\n",
    "encoder_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
