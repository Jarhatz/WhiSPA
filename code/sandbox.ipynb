{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperProcessor\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from config import WhiSBERTConfig, CACHE_DIR, CHECKPOINT_DIR\n",
    "from model import WhiSBERTModel\n",
    "from data import AudioDataset, collate_train\n",
    "from train import load_models\n",
    "from utils import (\n",
    "    mean_pooling,\n",
    "    cos_sim_loss,\n",
    "    sim_clr_loss,\n",
    "    norm_temp_ce_loss\n",
    ")\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>segment_filename</th>\n",
       "      <th>segment_message</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>ope</th>\n",
       "      <th>agr</th>\n",
       "      <th>ext</th>\n",
       "      <th>con</th>\n",
       "      <th>neu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-0</td>\n",
       "      <td>LS - Q5 - 0417-converted - 0000.wav</td>\n",
       "      <td>Over the last five years, what are the three ...</td>\n",
       "      <td>5.492248</td>\n",
       "      <td>2.434264</td>\n",
       "      <td>1.078491</td>\n",
       "      <td>3.538978</td>\n",
       "      <td>2.419177</td>\n",
       "      <td>-1.841604</td>\n",
       "      <td>-0.317128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-1</td>\n",
       "      <td>LS - Q5 - 0417-converted - 0001.wav</td>\n",
       "      <td>Over the last five years, I've had four grand...</td>\n",
       "      <td>5.345472</td>\n",
       "      <td>2.215654</td>\n",
       "      <td>5.080293</td>\n",
       "      <td>0.166491</td>\n",
       "      <td>-2.165736</td>\n",
       "      <td>-2.827166</td>\n",
       "      <td>1.538005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-2</td>\n",
       "      <td>LS - Q5 - 0417-converted - 0002.wav</td>\n",
       "      <td>In the last six years, I've had five grandchi...</td>\n",
       "      <td>5.345472</td>\n",
       "      <td>2.215654</td>\n",
       "      <td>5.080293</td>\n",
       "      <td>0.166491</td>\n",
       "      <td>-2.165736</td>\n",
       "      <td>-2.827166</td>\n",
       "      <td>1.538005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-3</td>\n",
       "      <td>LS - Q5 - 0417-converted - 0003.wav</td>\n",
       "      <td>Those are the nicest things that have happene...</td>\n",
       "      <td>5.862403</td>\n",
       "      <td>2.205863</td>\n",
       "      <td>1.145501</td>\n",
       "      <td>6.618364</td>\n",
       "      <td>4.583606</td>\n",
       "      <td>-1.471796</td>\n",
       "      <td>-0.623336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-4</td>\n",
       "      <td>LS - Q5 - 0417-converted - 0004.wav</td>\n",
       "      <td>Plus we've stayed together.</td>\n",
       "      <td>4.469452</td>\n",
       "      <td>1.632236</td>\n",
       "      <td>2.591754</td>\n",
       "      <td>1.388986</td>\n",
       "      <td>-0.087364</td>\n",
       "      <td>-7.546930</td>\n",
       "      <td>5.881711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154598</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-130</td>\n",
       "      <td>WTCHP Open Ended Linguistics - 1766-converted ...</td>\n",
       "      <td>Yes, lower the camera.</td>\n",
       "      <td>4.597764</td>\n",
       "      <td>2.151000</td>\n",
       "      <td>-1.472627</td>\n",
       "      <td>-1.503602</td>\n",
       "      <td>-2.542642</td>\n",
       "      <td>1.420707</td>\n",
       "      <td>0.526871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154599</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-131</td>\n",
       "      <td>WTCHP Open Ended Linguistics - 1766-converted ...</td>\n",
       "      <td>Keep the camera.</td>\n",
       "      <td>5.060868</td>\n",
       "      <td>2.280629</td>\n",
       "      <td>-1.724761</td>\n",
       "      <td>-0.399285</td>\n",
       "      <td>-0.700788</td>\n",
       "      <td>-2.833173</td>\n",
       "      <td>-0.502207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154600</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-132</td>\n",
       "      <td>WTCHP Open Ended Linguistics - 1766-converted ...</td>\n",
       "      <td>Maybe have a camera in the screen.</td>\n",
       "      <td>5.020720</td>\n",
       "      <td>2.168814</td>\n",
       "      <td>-1.478302</td>\n",
       "      <td>0.586663</td>\n",
       "      <td>-2.069125</td>\n",
       "      <td>-3.993055</td>\n",
       "      <td>0.031983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154601</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-133</td>\n",
       "      <td>WTCHP Open Ended Linguistics - 1766-converted ...</td>\n",
       "      <td>I think it would be a lot easier to answer th...</td>\n",
       "      <td>5.026930</td>\n",
       "      <td>2.495549</td>\n",
       "      <td>1.534325</td>\n",
       "      <td>3.015803</td>\n",
       "      <td>1.425991</td>\n",
       "      <td>2.239760</td>\n",
       "      <td>-3.279220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154602</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-134</td>\n",
       "      <td>WTCHP Open Ended Linguistics - 1766-converted ...</td>\n",
       "      <td>person's make eye contact with the individual.</td>\n",
       "      <td>5.418733</td>\n",
       "      <td>1.008181</td>\n",
       "      <td>3.512349</td>\n",
       "      <td>0.912426</td>\n",
       "      <td>-2.568315</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>-2.758698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154603 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    segment_id  \\\n",
       "0           417      417-Q5-0   \n",
       "1           417      417-Q5-1   \n",
       "2           417      417-Q5-2   \n",
       "3           417      417-Q5-3   \n",
       "4           417      417-Q5-4   \n",
       "...         ...           ...   \n",
       "154598     1766  1766--99-130   \n",
       "154599     1766  1766--99-131   \n",
       "154600     1766  1766--99-132   \n",
       "154601     1766  1766--99-133   \n",
       "154602     1766  1766--99-134   \n",
       "\n",
       "                                         segment_filename  \\\n",
       "0                     LS - Q5 - 0417-converted - 0000.wav   \n",
       "1                     LS - Q5 - 0417-converted - 0001.wav   \n",
       "2                     LS - Q5 - 0417-converted - 0002.wav   \n",
       "3                     LS - Q5 - 0417-converted - 0003.wav   \n",
       "4                     LS - Q5 - 0417-converted - 0004.wav   \n",
       "...                                                   ...   \n",
       "154598  WTCHP Open Ended Linguistics - 1766-converted ...   \n",
       "154599  WTCHP Open Ended Linguistics - 1766-converted ...   \n",
       "154600  WTCHP Open Ended Linguistics - 1766-converted ...   \n",
       "154601  WTCHP Open Ended Linguistics - 1766-converted ...   \n",
       "154602  WTCHP Open Ended Linguistics - 1766-converted ...   \n",
       "\n",
       "                                          segment_message   valence   arousal  \\\n",
       "0        Over the last five years, what are the three ...  5.492248  2.434264   \n",
       "1        Over the last five years, I've had four grand...  5.345472  2.215654   \n",
       "2        In the last six years, I've had five grandchi...  5.345472  2.215654   \n",
       "3        Those are the nicest things that have happene...  5.862403  2.205863   \n",
       "4                             Plus we've stayed together.  4.469452  1.632236   \n",
       "...                                                   ...       ...       ...   \n",
       "154598                             Yes, lower the camera.  4.597764  2.151000   \n",
       "154599                                   Keep the camera.  5.060868  2.280629   \n",
       "154600                 Maybe have a camera in the screen.  5.020720  2.168814   \n",
       "154601   I think it would be a lot easier to answer th...  5.026930  2.495549   \n",
       "154602     person's make eye contact with the individual.  5.418733  1.008181   \n",
       "\n",
       "             ope       agr       ext       con       neu  \n",
       "0       1.078491  3.538978  2.419177 -1.841604 -0.317128  \n",
       "1       5.080293  0.166491 -2.165736 -2.827166  1.538005  \n",
       "2       5.080293  0.166491 -2.165736 -2.827166  1.538005  \n",
       "3       1.145501  6.618364  4.583606 -1.471796 -0.623336  \n",
       "4       2.591754  1.388986 -0.087364 -7.546930  5.881711  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "154598 -1.472627 -1.503602 -2.542642  1.420707  0.526871  \n",
       "154599 -1.724761 -0.399285 -0.700788 -2.833173 -0.502207  \n",
       "154600 -1.478302  0.586663 -2.069125 -3.993055  0.031983  \n",
       "154601  1.534325  3.015803  1.425991  2.239760 -3.279220  \n",
       "154602  3.512349  0.912426 -2.568315  0.702222 -2.758698  \n",
       "\n",
       "[154603 rows x 11 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc_segments_df = pd.read_csv('/cronus_data/rrao/wtc_clinic/segment_outcomes.csv')\n",
    "print(wtc_segments_df[wtc_segments_df['segment_id'] == '417-Q5-0']['user_id'].values[0])\n",
    "wtc_segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>f000</th>\n",
       "      <th>f001</th>\n",
       "      <th>f002</th>\n",
       "      <th>f003</th>\n",
       "      <th>f004</th>\n",
       "      <th>f005</th>\n",
       "      <th>f006</th>\n",
       "      <th>f007</th>\n",
       "      <th>f008</th>\n",
       "      <th>...</th>\n",
       "      <th>f374</th>\n",
       "      <th>f375</th>\n",
       "      <th>f376</th>\n",
       "      <th>f377</th>\n",
       "      <th>f378</th>\n",
       "      <th>f379</th>\n",
       "      <th>f380</th>\n",
       "      <th>f381</th>\n",
       "      <th>f382</th>\n",
       "      <th>f383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417-Q5-0</td>\n",
       "      <td>-0.075584</td>\n",
       "      <td>0.040513</td>\n",
       "      <td>0.087918</td>\n",
       "      <td>-0.061818</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>-0.043687</td>\n",
       "      <td>-0.032497</td>\n",
       "      <td>-0.080257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031678</td>\n",
       "      <td>-0.036200</td>\n",
       "      <td>0.033863</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.063568</td>\n",
       "      <td>0.063140</td>\n",
       "      <td>0.052456</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.053543</td>\n",
       "      <td>0.059069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417-Q5-1</td>\n",
       "      <td>0.010263</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>0.052418</td>\n",
       "      <td>-0.054661</td>\n",
       "      <td>-0.052739</td>\n",
       "      <td>0.056436</td>\n",
       "      <td>-0.048300</td>\n",
       "      <td>-0.039810</td>\n",
       "      <td>-0.043792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.080140</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>0.011622</td>\n",
       "      <td>0.058191</td>\n",
       "      <td>0.037399</td>\n",
       "      <td>0.026069</td>\n",
       "      <td>0.044603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417-Q5-2</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>-0.039167</td>\n",
       "      <td>-0.044886</td>\n",
       "      <td>0.059167</td>\n",
       "      <td>-0.046924</td>\n",
       "      <td>-0.064981</td>\n",
       "      <td>-0.040909</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019591</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>0.043371</td>\n",
       "      <td>0.010914</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.022936</td>\n",
       "      <td>0.057911</td>\n",
       "      <td>0.041148</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.041009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417-Q5-3</td>\n",
       "      <td>-0.031706</td>\n",
       "      <td>0.083616</td>\n",
       "      <td>0.106976</td>\n",
       "      <td>-0.072090</td>\n",
       "      <td>-0.040272</td>\n",
       "      <td>-0.016612</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.031125</td>\n",
       "      <td>-0.003797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045717</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.034390</td>\n",
       "      <td>0.020780</td>\n",
       "      <td>0.058275</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.070823</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>0.046030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417-Q5-4</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>-0.013963</td>\n",
       "      <td>-0.037668</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>-0.027211</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080296</td>\n",
       "      <td>-0.086024</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>0.040391</td>\n",
       "      <td>-0.072536</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.019036</td>\n",
       "      <td>0.066409</td>\n",
       "      <td>-0.091498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154598</th>\n",
       "      <td>1766--99-130</td>\n",
       "      <td>0.097473</td>\n",
       "      <td>0.091618</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.079535</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.066491</td>\n",
       "      <td>0.025070</td>\n",
       "      <td>-0.140160</td>\n",
       "      <td>0.039958</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>-0.094962</td>\n",
       "      <td>-0.065360</td>\n",
       "      <td>0.065812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154599</th>\n",
       "      <td>1766--99-131</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>0.096108</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.110668</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>0.072415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044142</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.115293</td>\n",
       "      <td>0.045375</td>\n",
       "      <td>-0.089254</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>-0.040346</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.046477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154600</th>\n",
       "      <td>1766--99-132</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>0.091456</td>\n",
       "      <td>0.080520</td>\n",
       "      <td>-0.039176</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>-0.023656</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>-0.009069</td>\n",
       "      <td>0.061610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.012622</td>\n",
       "      <td>-0.056482</td>\n",
       "      <td>0.069467</td>\n",
       "      <td>-0.053189</td>\n",
       "      <td>-0.016472</td>\n",
       "      <td>-0.035657</td>\n",
       "      <td>0.024322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154601</th>\n",
       "      <td>1766--99-133</td>\n",
       "      <td>-0.015464</td>\n",
       "      <td>0.014041</td>\n",
       "      <td>0.052497</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>0.076290</td>\n",
       "      <td>-0.024053</td>\n",
       "      <td>-0.065092</td>\n",
       "      <td>-0.006300</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065374</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.036343</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>-0.013511</td>\n",
       "      <td>0.067152</td>\n",
       "      <td>0.055123</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>-0.089196</td>\n",
       "      <td>0.039081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154602</th>\n",
       "      <td>1766--99-134</td>\n",
       "      <td>-0.079210</td>\n",
       "      <td>0.014341</td>\n",
       "      <td>-0.012489</td>\n",
       "      <td>-0.020807</td>\n",
       "      <td>-0.042737</td>\n",
       "      <td>-0.113759</td>\n",
       "      <td>0.162280</td>\n",
       "      <td>-0.090367</td>\n",
       "      <td>-0.040929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050602</td>\n",
       "      <td>-0.043526</td>\n",
       "      <td>0.051272</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.054313</td>\n",
       "      <td>-0.074711</td>\n",
       "      <td>-0.058165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154603 rows Ã— 385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          segment_id      f000      f001      f002      f003      f004  \\\n",
       "0           417-Q5-0 -0.075584  0.040513  0.087918 -0.061818  0.016908   \n",
       "1           417-Q5-1  0.010263  0.028935  0.052418 -0.054661 -0.052739   \n",
       "2           417-Q5-2  0.015640  0.026896  0.055683 -0.039167 -0.044886   \n",
       "3           417-Q5-3 -0.031706  0.083616  0.106976 -0.072090 -0.040272   \n",
       "4           417-Q5-4  0.011252  0.017688  0.103457 -0.013963 -0.037668   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "154598  1766--99-130  0.097473  0.091618  0.018798  0.020551  0.004536   \n",
       "154599  1766--99-131  0.033504  0.096108  0.022334  0.002139  0.110668   \n",
       "154600  1766--99-132 -0.034302  0.091456  0.080520 -0.039176  0.015758   \n",
       "154601  1766--99-133 -0.015464  0.014041  0.052497  0.033327  0.076290   \n",
       "154602  1766--99-134 -0.079210  0.014341 -0.012489 -0.020807 -0.042737   \n",
       "\n",
       "            f005      f006      f007      f008  ...      f374      f375  \\\n",
       "0       0.054135 -0.043687 -0.032497 -0.080257  ...  0.031678 -0.036200   \n",
       "1       0.056436 -0.048300 -0.039810 -0.043792  ...  0.000716  0.005423   \n",
       "2       0.059167 -0.046924 -0.064981 -0.040909  ... -0.019591 -0.001969   \n",
       "3      -0.016612  0.007439  0.031125 -0.003797  ...  0.045717 -0.090909   \n",
       "4       0.025187 -0.000924 -0.027211  0.025800  ... -0.080296 -0.086024   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "154598  0.019850 -0.027589  0.020975  0.079535  ... -0.026897  0.008701   \n",
       "154599  0.023967  0.010173  0.026386  0.072415  ...  0.044142  0.008134   \n",
       "154600 -0.023656  0.001591 -0.009069  0.061610  ...  0.031829  0.013746   \n",
       "154601 -0.024053 -0.065092 -0.006300  0.003020  ... -0.065374  0.023941   \n",
       "154602 -0.113759  0.162280 -0.090367 -0.040929  ... -0.050602 -0.043526   \n",
       "\n",
       "            f376      f377      f378      f379      f380      f381      f382  \\\n",
       "0       0.033863  0.004600  0.063568  0.063140  0.052456  0.024611  0.053543   \n",
       "1       0.080140  0.005369  0.030937  0.011622  0.058191  0.037399  0.026069   \n",
       "2       0.043371  0.010914  0.013996  0.022936  0.057911  0.041148  0.008372   \n",
       "3       0.034390  0.020780  0.058275  0.102914  0.070823  0.082695 -0.009661   \n",
       "4       0.017136  0.019137  0.040391 -0.072536  0.018324  0.019036  0.066409   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "154598  0.066491  0.025070 -0.140160  0.039958  0.008782 -0.094962 -0.065360   \n",
       "154599  0.115293  0.045375 -0.089254  0.018317 -0.010786 -0.040346  0.000643   \n",
       "154600  0.113839  0.012622 -0.056482  0.069467 -0.053189 -0.016472 -0.035657   \n",
       "154601  0.036343  0.006751 -0.013511  0.067152  0.055123  0.009439 -0.089196   \n",
       "154602  0.051272 -0.025321  0.009834  0.013608  0.006367  0.054313 -0.074711   \n",
       "\n",
       "            f383  \n",
       "0       0.059069  \n",
       "1       0.044603  \n",
       "2       0.041009  \n",
       "3       0.046030  \n",
       "4      -0.091498  \n",
       "...          ...  \n",
       "154598  0.065812  \n",
       "154599  0.046477  \n",
       "154600  0.024322  \n",
       "154601  0.039081  \n",
       "154602 -0.058165  \n",
       "\n",
       "[154603 rows x 385 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = pd.read_csv('/cronus_data/rrao/WhiSBERT/embeddings/wtc_all-MiniLM-L12-v2.csv')\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = wtc_segments_df[['user_id', 'segment_id']].merge(feature_df, on='segment_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>segment_id</th>\n",
       "      <th>f000</th>\n",
       "      <th>f001</th>\n",
       "      <th>f002</th>\n",
       "      <th>f003</th>\n",
       "      <th>f004</th>\n",
       "      <th>f005</th>\n",
       "      <th>f006</th>\n",
       "      <th>f007</th>\n",
       "      <th>...</th>\n",
       "      <th>f374</th>\n",
       "      <th>f375</th>\n",
       "      <th>f376</th>\n",
       "      <th>f377</th>\n",
       "      <th>f378</th>\n",
       "      <th>f379</th>\n",
       "      <th>f380</th>\n",
       "      <th>f381</th>\n",
       "      <th>f382</th>\n",
       "      <th>f383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-0</td>\n",
       "      <td>-0.075584</td>\n",
       "      <td>0.040513</td>\n",
       "      <td>0.087918</td>\n",
       "      <td>-0.061818</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>-0.043687</td>\n",
       "      <td>-0.032497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031678</td>\n",
       "      <td>-0.036200</td>\n",
       "      <td>0.033863</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.063568</td>\n",
       "      <td>0.063140</td>\n",
       "      <td>0.052456</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>0.053543</td>\n",
       "      <td>0.059069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-1</td>\n",
       "      <td>0.010263</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>0.052418</td>\n",
       "      <td>-0.054661</td>\n",
       "      <td>-0.052739</td>\n",
       "      <td>0.056436</td>\n",
       "      <td>-0.048300</td>\n",
       "      <td>-0.039810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.080140</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>0.011622</td>\n",
       "      <td>0.058191</td>\n",
       "      <td>0.037399</td>\n",
       "      <td>0.026069</td>\n",
       "      <td>0.044603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-2</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>0.055683</td>\n",
       "      <td>-0.039167</td>\n",
       "      <td>-0.044886</td>\n",
       "      <td>0.059167</td>\n",
       "      <td>-0.046924</td>\n",
       "      <td>-0.064981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019591</td>\n",
       "      <td>-0.001969</td>\n",
       "      <td>0.043371</td>\n",
       "      <td>0.010914</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.022936</td>\n",
       "      <td>0.057911</td>\n",
       "      <td>0.041148</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.041009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-3</td>\n",
       "      <td>-0.031706</td>\n",
       "      <td>0.083616</td>\n",
       "      <td>0.106976</td>\n",
       "      <td>-0.072090</td>\n",
       "      <td>-0.040272</td>\n",
       "      <td>-0.016612</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.031125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045717</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.034390</td>\n",
       "      <td>0.020780</td>\n",
       "      <td>0.058275</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.070823</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>-0.009661</td>\n",
       "      <td>0.046030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>417</td>\n",
       "      <td>417-Q5-4</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>0.103457</td>\n",
       "      <td>-0.013963</td>\n",
       "      <td>-0.037668</td>\n",
       "      <td>0.025187</td>\n",
       "      <td>-0.000924</td>\n",
       "      <td>-0.027211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080296</td>\n",
       "      <td>-0.086024</td>\n",
       "      <td>0.017136</td>\n",
       "      <td>0.019137</td>\n",
       "      <td>0.040391</td>\n",
       "      <td>-0.072536</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.019036</td>\n",
       "      <td>0.066409</td>\n",
       "      <td>-0.091498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154598</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-130</td>\n",
       "      <td>0.097473</td>\n",
       "      <td>0.091618</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>0.004536</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.066491</td>\n",
       "      <td>0.025070</td>\n",
       "      <td>-0.140160</td>\n",
       "      <td>0.039958</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>-0.094962</td>\n",
       "      <td>-0.065360</td>\n",
       "      <td>0.065812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154599</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-131</td>\n",
       "      <td>0.033504</td>\n",
       "      <td>0.096108</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.110668</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044142</td>\n",
       "      <td>0.008134</td>\n",
       "      <td>0.115293</td>\n",
       "      <td>0.045375</td>\n",
       "      <td>-0.089254</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>-0.010786</td>\n",
       "      <td>-0.040346</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.046477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154600</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-132</td>\n",
       "      <td>-0.034302</td>\n",
       "      <td>0.091456</td>\n",
       "      <td>0.080520</td>\n",
       "      <td>-0.039176</td>\n",
       "      <td>0.015758</td>\n",
       "      <td>-0.023656</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>-0.009069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>0.012622</td>\n",
       "      <td>-0.056482</td>\n",
       "      <td>0.069467</td>\n",
       "      <td>-0.053189</td>\n",
       "      <td>-0.016472</td>\n",
       "      <td>-0.035657</td>\n",
       "      <td>0.024322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154601</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-133</td>\n",
       "      <td>-0.015464</td>\n",
       "      <td>0.014041</td>\n",
       "      <td>0.052497</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>0.076290</td>\n",
       "      <td>-0.024053</td>\n",
       "      <td>-0.065092</td>\n",
       "      <td>-0.006300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065374</td>\n",
       "      <td>0.023941</td>\n",
       "      <td>0.036343</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>-0.013511</td>\n",
       "      <td>0.067152</td>\n",
       "      <td>0.055123</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>-0.089196</td>\n",
       "      <td>0.039081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154602</th>\n",
       "      <td>1766</td>\n",
       "      <td>1766--99-134</td>\n",
       "      <td>-0.079210</td>\n",
       "      <td>0.014341</td>\n",
       "      <td>-0.012489</td>\n",
       "      <td>-0.020807</td>\n",
       "      <td>-0.042737</td>\n",
       "      <td>-0.113759</td>\n",
       "      <td>0.162280</td>\n",
       "      <td>-0.090367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050602</td>\n",
       "      <td>-0.043526</td>\n",
       "      <td>0.051272</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.054313</td>\n",
       "      <td>-0.074711</td>\n",
       "      <td>-0.058165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154603 rows Ã— 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    segment_id      f000      f001      f002      f003  \\\n",
       "0           417      417-Q5-0 -0.075584  0.040513  0.087918 -0.061818   \n",
       "1           417      417-Q5-1  0.010263  0.028935  0.052418 -0.054661   \n",
       "2           417      417-Q5-2  0.015640  0.026896  0.055683 -0.039167   \n",
       "3           417      417-Q5-3 -0.031706  0.083616  0.106976 -0.072090   \n",
       "4           417      417-Q5-4  0.011252  0.017688  0.103457 -0.013963   \n",
       "...         ...           ...       ...       ...       ...       ...   \n",
       "154598     1766  1766--99-130  0.097473  0.091618  0.018798  0.020551   \n",
       "154599     1766  1766--99-131  0.033504  0.096108  0.022334  0.002139   \n",
       "154600     1766  1766--99-132 -0.034302  0.091456  0.080520 -0.039176   \n",
       "154601     1766  1766--99-133 -0.015464  0.014041  0.052497  0.033327   \n",
       "154602     1766  1766--99-134 -0.079210  0.014341 -0.012489 -0.020807   \n",
       "\n",
       "            f004      f005      f006      f007  ...      f374      f375  \\\n",
       "0       0.016908  0.054135 -0.043687 -0.032497  ...  0.031678 -0.036200   \n",
       "1      -0.052739  0.056436 -0.048300 -0.039810  ...  0.000716  0.005423   \n",
       "2      -0.044886  0.059167 -0.046924 -0.064981  ... -0.019591 -0.001969   \n",
       "3      -0.040272 -0.016612  0.007439  0.031125  ...  0.045717 -0.090909   \n",
       "4      -0.037668  0.025187 -0.000924 -0.027211  ... -0.080296 -0.086024   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "154598  0.004536  0.019850 -0.027589  0.020975  ... -0.026897  0.008701   \n",
       "154599  0.110668  0.023967  0.010173  0.026386  ...  0.044142  0.008134   \n",
       "154600  0.015758 -0.023656  0.001591 -0.009069  ...  0.031829  0.013746   \n",
       "154601  0.076290 -0.024053 -0.065092 -0.006300  ... -0.065374  0.023941   \n",
       "154602 -0.042737 -0.113759  0.162280 -0.090367  ... -0.050602 -0.043526   \n",
       "\n",
       "            f376      f377      f378      f379      f380      f381      f382  \\\n",
       "0       0.033863  0.004600  0.063568  0.063140  0.052456  0.024611  0.053543   \n",
       "1       0.080140  0.005369  0.030937  0.011622  0.058191  0.037399  0.026069   \n",
       "2       0.043371  0.010914  0.013996  0.022936  0.057911  0.041148  0.008372   \n",
       "3       0.034390  0.020780  0.058275  0.102914  0.070823  0.082695 -0.009661   \n",
       "4       0.017136  0.019137  0.040391 -0.072536  0.018324  0.019036  0.066409   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "154598  0.066491  0.025070 -0.140160  0.039958  0.008782 -0.094962 -0.065360   \n",
       "154599  0.115293  0.045375 -0.089254  0.018317 -0.010786 -0.040346  0.000643   \n",
       "154600  0.113839  0.012622 -0.056482  0.069467 -0.053189 -0.016472 -0.035657   \n",
       "154601  0.036343  0.006751 -0.013511  0.067152  0.055123  0.009439 -0.089196   \n",
       "154602  0.051272 -0.025321  0.009834  0.013608  0.006367  0.054313 -0.074711   \n",
       "\n",
       "            f383  \n",
       "0       0.059069  \n",
       "1       0.044603  \n",
       "2       0.041009  \n",
       "3       0.046030  \n",
       "4      -0.091498  \n",
       "...          ...  \n",
       "154598  0.065812  \n",
       "154599  0.046477  \n",
       "154600  0.024322  \n",
       "154601  0.039081  \n",
       "154602 -0.058165  \n",
       "\n",
       "[154603 rows x 386 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'f383', 0.0013391561822598491, 0.0013391561822598491)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "for user_id in np.unique(merged_df['user_id']):\n",
    "    mean_feats = merged_df[merged_df['user_id'] == user_id].iloc[:, 2:].mean()\n",
    "    for feat_name, value in mean_feats.items():\n",
    "        values = (user_id, feat_name, value, value)\n",
    "    break\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/cronus_data/rrao/WhiSBERT/embeddings/all-MiniLM-L12-v2.csv')\n",
    "hitop_df = df[:-154603]\n",
    "wtc_df = df[-154603:].reset_index(drop=True)\n",
    "# hitop_df.to_csv('/cronus_data/rrao/WhiSBERT/embeddings/hitop_all-MiniLM-L12-v2.csv', index=False)\n",
    "# wtc_df.to_csv('/cronus_data/rrao/WhiSBERT/embeddings/wtc_all-MiniLM-L12-v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence.\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1', cache_folder=CACHE_DIR)\n",
    "# model = torch.nn.DataParallel(model, device_ids=[1,2,3])\n",
    "embeddings = torch.from_numpy(model.encode(sentences))\n",
    "print(embeddings.shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8855)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].auto_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (layer): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].auto_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('activation_function', Tanh()),\n",
       "              ('linear',\n",
       "               Linear(in_features=768, out_features=512, bias=True))]),\n",
       " 'in_features': 768,\n",
       " 'out_features': 512,\n",
       " 'bias': True}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=512, bias=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tanh()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = WhiSBERTConfig(\n",
    "    whisper_model_id='openai/whisper-base',\n",
    "    pooling_mode='mean',\n",
    "    use_sbert_encoder=True,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    device='cuda'\n",
    ")\n",
    "processor, _, _, _ = load_models(config, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing AudioDataset...\n",
      "\tTotal dataset size (N): 50369\n",
      "\tTraining dataset size (N): 40295\n",
      "\tValidation dataset size (N): 10074\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing AudioDataset...')\n",
    "dataset = AudioDataset(processor)\n",
    "mini_size = int(0.1 * len(dataset))\n",
    "drop_size = len(dataset) - mini_size\n",
    "mini_dataset, _ = torch.utils.data.random_split(dataset, [mini_size, drop_size])\n",
    "\n",
    "# Calculate lengths for the train/val split (80:20)\n",
    "total_size = len(mini_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "# Perform the split\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mini_dataset, [train_size, val_size])\n",
    "print(f'\\tTotal dataset size (N): {total_size}')\n",
    "print(f'\\tTraining dataset size (N): {train_size}')\n",
    "print(f'\\tValidation dataset size (N): {val_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=config.shuffle,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 80, 3000])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print(batch['audio_inputs'].shape)\n",
    "print(len(batch['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt').to(config.device)\n",
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 28, 768])\n",
      "torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sbert_output = sbert(**encoded_input)\n",
    "print(sbert_output.last_hidden_state.shape)\n",
    "sentence_embeddings = mean_pooling(sbert_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 22, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = sbert.embeddings(input_ids=encoded_input['input_ids'])#, attn_mask=encoded_input['attention_mask'])\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 22, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask = [None] * sbert.config.num_hidden_layers\n",
    "encoder_output = sbert.transformer(\n",
    "    embedding_output,\n",
    "    attn_mask=torch.ones(encoded_input['input_ids'].size(), device=config.device),\n",
    "    head_mask=head_mask\n",
    ")[0]\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel.from_pretrained(\n",
    "    config.whisper_model_id,\n",
    "    cache_dir=CACHE_DIR\n",
    ").to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 27])\n",
      "torch.Size([8, 27])\n"
     ]
    }
   ],
   "source": [
    "# Whisper-based tokenization\n",
    "with torch.no_grad():\n",
    "    outputs = processor.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    ).to(config.device)\n",
    "print(outputs['input_ids'].shape)\n",
    "print(outputs['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = whisper_model(\n",
    "    batch['audio_inputs'].to(config.device),\n",
    "    decoder_input_ids=outputs['input_ids'],\n",
    "    decoder_attention_mask=outputs['attention_mask']\n",
    ").last_hidden_state\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], whisper_embs.size()[:-1])\n",
    "# extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], outputs['attention_mask'].size())\n",
    "encoder_output = sbert_model.transformer(embedding_output, attn_mask=torch.ones(encoded_input['input_ids'].size(), device=config.device), head_mask=head_mask)[0]\n",
    "encoder_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
