{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperProcessor,\n",
    "    WhisperModel\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from config import WhiSBERTConfig\n",
    "from utils import (\n",
    "    mean_pooling,\n",
    "    last_pooling,\n",
    "    cos_sim_loss,\n",
    "    clr_cos_loss,\n",
    "    sim_clr_loss\n",
    ")\n",
    "from data import AudioDataset, collate\n",
    "from train import load_models, train\n",
    "\n",
    "CACHE_DIR = '/cronus_data/rrao/cache'\n",
    "CHECKPOINT_DIR = '/cronus_data/rrao/WhisBERT/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch of unlabeled data, v1 to v4: \n",
      "[0.5 0.6 0.5 0.6]\n",
      "[0.1 0.1 0.2 0.2]\n",
      "[0.9 0.8 0.9 0.8]\n",
      "[0.3 0.7 0.7 0.3]\n",
      "\n",
      "Augmented data, v5 to v8: \n",
      "[0.55 0.65 0.5  0.6 ]\n",
      "[0.1  0.15 0.25 0.2 ]\n",
      "[0.9  0.85 0.95 0.8 ]\n",
      "[0.35 0.7  0.75 0.3 ]\n"
     ]
    }
   ],
   "source": [
    "# a batch of data\n",
    "v1 = np.array([0.5, 0.6, 0.5, 0.6])\n",
    "v2 = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "v3 = np.array([0.9, 0.8, 0.9, 0.8])\n",
    "v4 = np.array([0.3, 0.7, 0.7, 0.3])\n",
    "\n",
    "print(\"\\nBatch of unlabeled data, v1 to v4: \")\n",
    "print(v1); print(v2); print(v3); print(v4)\n",
    "\n",
    "# augmented data\n",
    "v5 = np.array([0.55, 0.65, 0.50, 0.60])  # from v1\n",
    "v6 = np.array([0.10, 0.15, 0.25, 0.20])  # from v2\n",
    "v7 = np.array([0.90, 0.85, 0.95, 0.80])  # from v3\n",
    "v8 = np.array([0.35, 0.70, 0.75, 0.30])  # from v4\n",
    "\n",
    "print(\"\\nAugmented data, v5 to v8: \")\n",
    "print(v5); print(v6); print(v7); print(v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "whis_embs = torch.from_numpy(np.stack([v1, v2, v3, v4]))\n",
    "sbert_embs = torch.from_numpy(np.stack([v5, v6, v7, v8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(whis_embs, sbert_embs, tau=0.10):\n",
    "    # Helpful link I used for reference:\n",
    "    # https://jamesmccaffrey.wordpress.com/2022/04/11/an-example-of-normalized-temperature-scaled-cross-entropy-loss/\n",
    "    z_audio = F.normalize(whis_embs, dim=1)\n",
    "    z_text = F.normalize(sbert_embs, dim=1)\n",
    "    \n",
    "    # Compute cosine similarity for all pairs in the batch\n",
    "    similarity_matrix = torch.matmul(z_audio, z_text.T) / tau\n",
    "    similarity_matrix = torch.exp(similarity_matrix)\n",
    "\n",
    "    # Sum over each row, excluding the diagonal (self-similarity terms)\n",
    "    pos_sims = torch.diag(similarity_matrix)\n",
    "    neg_sims_sum = similarity_matrix.sum(dim=1) - pos_sims\n",
    "    print(neg_sims_sum)\n",
    "    \n",
    "    losses = -torch.log(pos_sims / neg_sims_sum)\n",
    "    return losses.sum()\n",
    "\n",
    "\n",
    "def b(whis_embs, sbert_embs, tau=0.10):\n",
    "    # Helpful link I used for reference:\n",
    "    # https://jamesmccaffrey.wordpress.com/2022/04/11/an-example-of-normalized-temperature-scaled-cross-entropy-loss/\n",
    "    combined = torch.cat([whis_embs, sbert_embs], dim=0)  # shape (2 * batch_size, emb_dim)\n",
    "    combined = F.normalize(combined, dim=1)\n",
    "\n",
    "    # Define positive pairs (each original data with its corresponding augmented data)\n",
    "    batch_size = whis_embs.shape[0]\n",
    "    pos_pairs = torch.arange(batch_size)\n",
    "    pos_indices = pos_pairs + batch_size  # offset by batch_size to point to the sbert_embs\n",
    "    \n",
    "    # Compute cosine similarity for all pairs in the batch\n",
    "    similarity_matrix = torch.matmul(combined, combined.T) / tau\n",
    "    similarity_matrix = torch.exp(similarity_matrix)\n",
    "    \n",
    "    pos_sims = similarity_matrix[pos_pairs, pos_indices]\n",
    "    neg_sims_sum = similarity_matrix[:batch_size].sum(dim=1) - torch.diag(similarity_matrix[:batch_size])\n",
    "    print(neg_sims_sum)\n",
    "    \n",
    "    losses = -torch.log(pos_sims / neg_sims_sum)\n",
    "    return losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6910, dtype=torch.float64)\n",
      "Elapsed time: 0.03089737892150879 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(clr_cos_loss(whis_embs, sbert_embs))\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21819.4160, 19883.0742, 21936.7833, 21780.0413], dtype=torch.float64)\n",
      "tensor([43334.7961, 30951.2489, 44569.1498, 33562.6894], dtype=torch.float64)\n",
      "tensor(2.2700, dtype=torch.float64)\n",
      "Elapsed time: 0.0026175975799560547 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(a(whis_embs, sbert_embs))\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21819.4160, 19883.0742, 21936.7833, 21780.0413], dtype=torch.float64)\n",
      "tensor([108116.1506,  85319.3133, 109872.0390,  83249.7785],\n",
      "       dtype=torch.float64)\n",
      "tensor(6.0089, dtype=torch.float64)\n",
      "Elapsed time: 0.003621339797973633 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(b(whis_embs, sbert_embs))\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss for positive pair v1,v5 \n",
      "\n",
      "1.598489\n"
     ]
    }
   ],
   "source": [
    "def sim(v1, v2):\n",
    "  v1_normed = v1 / np.linalg.norm(v1)\n",
    "  v2_normed = v2 / np.linalg.norm(v2)\n",
    "  return np.dot(v1_normed, v2_normed)  # normalized dot prod\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "tau = 0.10  # temperature\n",
    "\n",
    "print(\"Computing loss for positive pair v1,v5 \")\n",
    "# loss for positive pair (v1, v5)\n",
    "v1v1 = np.exp(sim(v1,v1)/tau)  # not used\n",
    "v1v2 = np.exp(sim(v1,v2)/tau)\n",
    "v1v3 = np.exp(sim(v1,v3)/tau)\n",
    "v1v4 = np.exp(sim(v1,v4)/tau)\n",
    "v1v5 = np.exp(sim(v1,v5)/tau)  # should be small\n",
    "v1v6 = np.exp(sim(v1,v6)/tau)\n",
    "v1v7 = np.exp(sim(v1,v7)/tau)\n",
    "v1v8 = np.exp(sim(v1,v8)/tau)\n",
    "\n",
    "numerator = v1v5\n",
    "denom = v1v2 + v1v3 + v1v4 + v1v5 + v1v6 + v1v7 + v1v8\n",
    "\n",
    "loss_v1v5 = -np.log(numerator / denom)\n",
    "print(\"\\n%0.6f\" % loss_v1v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1', cache_folder=CACHE_DIR)\n",
    "# model = torch.nn.DataParallel(model, device_ids=[1,2,3])\n",
    "embeddings = torch.from_numpy(model.encode(sentences))\n",
    "print(embeddings.shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(model, torch.nn.DataParallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].auto_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (layer): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attention): MultiHeadSelfAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (ffn): FFN(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].auto_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('activation_function', Tanh()),\n",
       "              ('linear',\n",
       "               Linear(in_features=768, out_features=512, bias=True))]),\n",
       " 'in_features': 768,\n",
       " 'out_features': 512,\n",
       " 'bias': True}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=512, bias=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tanh()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[2].activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = WhiSBERTConfig(\n",
    "    whisper_model_id='openai/whisper-base',\n",
    "    pooling_mode='mean',\n",
    "    use_sbert_encoder=True,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    device='cuda'\n",
    ")\n",
    "processor, _, _, _ = load_models(config, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing AudioDataset...\n",
      "\tTotal dataset size (N): 50369\n",
      "\tTraining dataset size (N): 40295\n",
      "\tValidation dataset size (N): 10074\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing AudioDataset...')\n",
    "dataset = AudioDataset(processor)\n",
    "mini_size = int(0.1 * len(dataset))\n",
    "drop_size = len(dataset) - mini_size\n",
    "mini_dataset, _ = torch.utils.data.random_split(dataset, [mini_size, drop_size])\n",
    "\n",
    "# Calculate lengths for the train/val split (80:20)\n",
    "total_size = len(mini_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "# Perform the split\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mini_dataset, [train_size, val_size])\n",
    "print(f'\\tTotal dataset size (N): {total_size}')\n",
    "print(f'\\tTraining dataset size (N): {train_size}')\n",
    "print(f'\\tValidation dataset size (N): {val_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=config.shuffle,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 80, 3000])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print(batch['audio_inputs'].shape)\n",
    "print(len(batch['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt').to(config.device)\n",
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 28, 768])\n",
      "torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sbert_output = sbert(**encoded_input)\n",
    "print(sbert_output.last_hidden_state.shape)\n",
    "sentence_embeddings = mean_pooling(sbert_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 22, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = sbert.embeddings(input_ids=encoded_input['input_ids'])#, attn_mask=encoded_input['attention_mask'])\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 22, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_mask = [None] * sbert.config.num_hidden_layers\n",
    "encoder_output = sbert.transformer(\n",
    "    embedding_output,\n",
    "    attn_mask=torch.ones(encoded_input['input_ids'].size(), device=config.device),\n",
    "    head_mask=head_mask\n",
    ")[0]\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = WhisperModel.from_pretrained(\n",
    "    config.whisper_model_id,\n",
    "    cache_dir=CACHE_DIR\n",
    ").to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 27])\n",
      "torch.Size([8, 27])\n"
     ]
    }
   ],
   "source": [
    "# Whisper-based tokenization\n",
    "with torch.no_grad():\n",
    "    outputs = processor.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    ).to(config.device)\n",
    "print(outputs['input_ids'].shape)\n",
    "print(outputs['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = whisper_model(\n",
    "    batch['audio_inputs'].to(config.device),\n",
    "    decoder_input_ids=outputs['input_ids'],\n",
    "    decoder_attention_mask=outputs['attention_mask']\n",
    ").last_hidden_state\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], whisper_embs.size()[:-1])\n",
    "# extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], outputs['attention_mask'].size())\n",
    "encoder_output = sbert_model.transformer(embedding_output, attn_mask=torch.ones(encoded_input['input_ids'].size(), device=config.device), head_mask=head_mask)[0]\n",
    "encoder_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
