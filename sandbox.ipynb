{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    WhisperProcessor,\n",
    "    WhisperModel\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "from train import load_models, train\n",
    "from utils import WhiSBERTConfig, AudioDataset, collate, mean_pooling\n",
    "\n",
    "CACHE_DIR = '/cronus_data/rrao/cache'\n",
    "CHECKPOINT_DIR = '/cronus_data/rrao/WhisBERT/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available GPU IDs: [0, 1, 2, 3]\n",
      "\tGPU 0: NVIDIA RTX A6000\n",
      "\tGPU 1: NVIDIA RTX A6000\n",
      "\tGPU 2: NVIDIA RTX A6000\n",
      "\tGPU 3: NVIDIA RTX A6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = WhiSBERTConfig(pooling_mode='cls', use_sbert_layers=False, batch_size=8, shuffle=False, device='cuda')\n",
    "processor, whisbert, tokenizer, sbert = load_models(config, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing AudioDataset...\n",
      "\tTotal dataset size (N): 15518\n",
      "\tTraining dataset size (N): 12414\n",
      "\tValidation dataset size (N): 3104\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing AudioDataset...')\n",
    "dataset = AudioDataset('/cronus_data/rrao/wtc_clinic/whisper_segments_transripts.csv', processor)\n",
    "mini_size = int(0.1 * len(dataset))\n",
    "drop_size = len(dataset) - mini_size\n",
    "mini_dataset, _ = torch.utils.data.random_split(dataset, [mini_size, drop_size])\n",
    "\n",
    "# Calculate lengths for the train/val split (80:20)\n",
    "total_size = len(mini_dataset)\n",
    "train_size = int(0.8 * total_size)  # 80% for training\n",
    "val_size = total_size - train_size  # 20% for validation\n",
    "# Perform the split\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mini_dataset, [train_size, val_size])\n",
    "print(f'\\tTotal dataset size (N): {total_size}')\n",
    "print(f'\\tTraining dataset size (N): {train_size}')\n",
    "print(f'\\tValidation dataset size (N): {val_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=config.shuffle,\n",
    "    collate_fn=collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 80, 3000])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "print(batch['audio_inputs'].shape)\n",
    "print(len(batch['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper-based tokenization\n",
    "with torch.no_grad():\n",
    "    outputs = processor.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    ).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whis_embs = whisbert(\n",
    "    batch['audio_inputs'].to(config.device),\n",
    "    outputs['input_ids'],\n",
    "    outputs['attention_mask']\n",
    ")\n",
    "whis_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sbert_model_id = 'sentence-transformers/all-mpnet-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(sbert_model_id, cache_dir=CACHE_DIR)\n",
    "sbert_model = AutoModel.from_pretrained(sbert_model_id, cache_dir=CACHE_DIR).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt').to(config.device)\n",
    "encoded_input['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.LongTensor{[8, 32, 1]}, size=[32, 768]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     sbert_output \u001b[38;5;241m=\u001b[39m sbert_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\n\u001b[0;32m----> 3\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmean_pooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43msbert_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence_embeddings\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/workspace/WhiSBERT/utils.py:125\u001b[0m, in \u001b[0;36mmean_pooling\u001b[0;34m(model_output, attention_mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_pooling\u001b[39m(model_output, attention_mask):\n\u001b[1;32m    124\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# First element of model_output contains all token embeddings\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     input_mask_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(token_embeddings \u001b[38;5;241m*\u001b[39m input_mask_expanded, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(input_mask_expanded\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.cuda.LongTensor{[8, 32, 1]}, size=[32, 768]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sbert_output = sbert_model(**encoded_input)\n",
    "sentence_embeddings = mean_pooling(sbert_output.last_hidden_state, encoded_input['attention_mask'])\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 17, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output = sbert_model.embeddings(input_ids=encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])\n",
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_attention_mask = sbert_model.get_extended_attention_mask(outputs['attention_mask'], whisper_embs.size()[:-1])\n",
    "head_mask = [None] * sbert_model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 18, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output = sbert_model.encoder(whisper_embs, attention_mask=extended_attention_mask, head_mask=head_mask)[0]\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output = sbert_model.pooler(encoder_output)\n",
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and model\n",
    "whisper_name = \"openai/whisper-small\"\n",
    "whisper_processor = WhisperProcessor.from_pretrained(whisper_name, cache_dir='/cronus_data/rrao/cache/')\n",
    "# whisper_generator = WhisperForConditionalGeneration.from_pretrained(whisper_name, cache_dir='/cronus_data/rrao/cache/')\n",
    "whisper_model = WhisperModel.from_pretrained(whisper_name, cache_dir='/cronus_data/rrao/cache/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    'segment_filename': [\n",
    "        'P209_segment.wav',\n",
    "        'P360_segment.wav',\n",
    "        'P443_segment.wav',\n",
    "        'PP636_segment.wav'\n",
    "    ],\n",
    "    'text': [\n",
    "        ' Nothing different, the same.',\n",
    "        ' Yeah, like I like men.',\n",
    "        \" Biology doesn't change what the mind says.\",\n",
    "        ' esta man, bisexual.'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 3000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.cat([preprocess_audio(whisper_processor, os.path.join('/cronus_data/rrao/samples', audio_filename)) for audio_filename in batch['segment_filename']], dim=0)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "token_type = 'cls'\n",
    "\n",
    "# Put models in evaluation mode\n",
    "# whisper_generator.eval()\n",
    "whisper_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # generated_ids = whisper_generator.generate(inputs)\n",
    "    outputs = whisper_processor.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    embs = whisper_model(inputs, decoder_input_ids=outputs['input_ids'], decoder_attention_mask=outputs['attention_mask']).last_hidden_state\n",
    "    \n",
    "    if token_type == 'cls':\n",
    "        non_padding_indices = outputs['attention_mask'].cumsum(dim=1) - 1\n",
    "        last_non_padding_indices = non_padding_indices.gather(1, (outputs['attention_mask'].sum(dim=1, keepdim=True) - 1).clamp(min=0).long())\n",
    "        embs = embs[torch.arange(outputs['attention_mask'].size(0)).unsqueeze(1), last_non_padding_indices].squeeze()\n",
    "    else:\n",
    "        sum_embs = (embs * outputs['attention_mask'].unsqueeze(-1).expand(embs.size())).sum(dim=1)\n",
    "        non_padding_counts = outputs['attention_mask'].sum(dim=1).unsqueeze(-1).clamp(min=1)\n",
    "        embs = sum_embs / non_padding_counts\n",
    "    \n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cronus_data/rrao/conda_envs/speech/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = WhisBERTConfig(token_type='cls', use_new_encoder_layers=False)\n",
    "_, whisbert_model, _, _ = load_models(config, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "whisbert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = whisper_processor.tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    whis_embs = whisbert_model(inputs, text_input_ids=outputs['input_ids'], text_attention_mask=outputs['attention_mask'])\n",
    "\n",
    "print(whis_embs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
